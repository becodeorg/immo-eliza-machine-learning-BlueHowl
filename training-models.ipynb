{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af752143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, root_mean_squared_error, r2_score\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f49cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the CSV file\n",
    "df = pd.read_csv(\"./data/train_data-cleaned.csv\")\n",
    "df_test = pd.read_csv(\"./data/test_data-cleaned.csv\")\n",
    "\n",
    "# Display the first few rows to verify the data was loaded correctly\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c783b5",
   "metadata": {},
   "source": [
    "Prepare for regression + Standardize scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911ef947",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_correlations = df.corr()['price'].abs().sort_values(ascending=False)\n",
    "features = price_correlations.index.tolist()[1:]\n",
    "\n",
    "target = 'price'\n",
    "\n",
    "x_train = df[features]\n",
    "y_train = df[target]\n",
    "\n",
    "# Split the dataset\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "x_test = df_test[features]\n",
    "y_test = df_test[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccf9112",
   "metadata": {},
   "source": [
    "---\n",
    "## Option 1: Linear Regression Coefficients (for linear relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6073474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(x_train, y_train)\n",
    "\n",
    "# Get importance from coefficients (absolute values give relative importance)\n",
    "importance = pd.Series(np.abs(lr.coef_), index=features)\n",
    "importance.sort_values(ascending=False).plot(kind='barh', title='Linear Regression Feature Importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7481503c",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b1730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(x_test)\n",
    "print(\"RMSE:\", root_mean_squared_error(y_test, y_pred))\n",
    "print(\"R² Score:\", r2_score(y_test, y_pred))\n",
    "print(\"MAPE:\", np.mean(np.abs((y_test - y_pred) / y_test)) * 100, \"%\")\n",
    "print(\"MAE:\", np.mean(np.abs(y_test - y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a6b5f4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Option 2: XGBoost\n",
    "XGBoost (eXtreme Gradient Boosting) is another powerful ensemble learning technique that often outperforms both linear regression and random forest models. It uses gradient boosting with regularization to reduce overfitting and improve performance.\n",
    "\n",
    "Key advantages of XGBoost:\n",
    "- High performance and efficient computation\n",
    "- Regularization to prevent overfitting\n",
    "- Handles missing values automatically\n",
    "- Supports parallel processing\n",
    "- Offers built-in cross-validation\n",
    "\n",
    "For our housing price prediction task, XGBoost can capture complex non-linear relationships while maintaining good generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b136ac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize XGBoost model with good default parameters\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.2,\n",
    "    max_depth=7,\n",
    "    min_child_weight=1,\n",
    "    subsample=1.0,\n",
    "    colsample_bytree=0.6,\n",
    "    objective='reg:squarederror',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# # Finetuning with GridSearchCV,\n",
    "# param_grid = {\n",
    "#     'n_estimators': [50, 100, 200],\n",
    "#     'max_depth': [3, 5, 7],\n",
    "#     'learning_rate': [0.01, 0.1, 0.2],\n",
    "#     'subsample': [0.6, 0.8, 1.0],\n",
    "#     'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "#     'min_child_weight': [1, 3, 5]\n",
    "# }\n",
    "# grid_search = GridSearchCV(\n",
    "#     estimator=xgb.XGBRegressor(objective='reg:squarederror', random_state=42),\n",
    "#     param_grid=param_grid,\n",
    "#     scoring='neg_mean_squared_error',\n",
    "#     cv=3,\n",
    "#     verbose=1,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "# grid_search.fit(x_train, y_train)\n",
    "# best_params = grid_search.best_params_\n",
    "# print(f\"Best parameters: {best_params}\")\n",
    "# xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, **best_params)\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(x_train, y_train)\n",
    "\n",
    "# Get feature importance\n",
    "importance = pd.Series(xgb_model.feature_importances_, index=features)\n",
    "importance.sort_values(ascending=True).plot(\n",
    "    kind='barh', \n",
    "    title='XGBoost Feature Importance'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49788b21",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf633d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = xgb_model.predict(x_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"XGBoost Performance Metrics:\")\n",
    "print(\"RMSE:\", root_mean_squared_error(y_test, y_pred))\n",
    "print(\"R² Score:\", r2_score(y_test, y_pred))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(y_test, y_pred) * 100, \"%\")\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971a01b9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Option 3 : Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35065b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features for better neural network performance\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# Build the neural network model\n",
    "nn_model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(len(features),)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "nn_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.003),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = nn_model.fit(\n",
    "    x_train_scaled, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = nn_model.predict(x_test_scaled).flatten()\n",
    "print(\"\\nNeural Network Performance Metrics:\")\n",
    "print(\"RMSE:\", root_mean_squared_error(y_test, y_pred))\n",
    "print(\"R² Score:\", r2_score(y_test, y_pred))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(y_test, y_pred) * 100, \"%\")\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss During Training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Training MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.title('MAE During Training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd48a8f8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Option 4 : K-nearest neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bfa55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features for better KNN performance\n",
    "scaler_knn = StandardScaler()\n",
    "x_train_scaled_knn = scaler_knn.fit_transform(x_train)\n",
    "x_test_scaled_knn = scaler_knn.transform(x_test)\n",
    "\n",
    "# Simple KNN model with default parameters\n",
    "knn_model = KNeighborsRegressor(n_neighbors=5)\n",
    "knn_model.fit(x_train_scaled_knn, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_knn = knn_model.predict(x_test_scaled_knn)\n",
    "print(\"KNN Default Performance Metrics:\")\n",
    "print(\"RMSE:\", root_mean_squared_error(y_test, y_pred_knn))\n",
    "print(\"R² Score:\", r2_score(y_test, y_pred_knn))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(y_test, y_pred_knn) * 100, \"%\")\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_pred_knn))\n",
    "\n",
    "# Find optimal hyperparameters using GridSearchCV\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'p': [1, 2]  # p=1 is Manhattan, p=2 is Euclidean\n",
    "}\n",
    "\n",
    "# Only use a subset of data for GridSearchCV to make it faster\n",
    "grid_search = GridSearchCV(\n",
    "    KNeighborsRegressor(),\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train on a sample for speed (optional)\n",
    "grid_search.fit(x_train_scaled_knn[:10000], y_train[:10000])\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"\\nBest parameters: {best_params}\")\n",
    "\n",
    "# Train the optimized model\n",
    "best_knn = KNeighborsRegressor(**best_params)\n",
    "best_knn.fit(x_train_scaled_knn, y_train)\n",
    "y_pred_best_knn = best_knn.predict(x_test_scaled_knn)\n",
    "\n",
    "# Evaluate the optimized model\n",
    "print(\"\\nOptimized KNN Performance Metrics:\")\n",
    "print(\"RMSE:\", root_mean_squared_error(y_test, y_pred_best_knn))\n",
    "print(\"R² Score:\", r2_score(y_test, y_pred_best_knn))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(y_test, y_pred_best_knn) * 100, \"%\")\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_pred_best_knn))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
